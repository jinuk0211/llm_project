pagedattention

https://github.com/vllm-project/vllm/blob/main/docs/source/dev/kernel/paged_attention.rst

GQA

https://github.com/fkodom/grouped-query-attention-pytorch/blob/main/grouped_query_attention_pytorch/attention.py

speculative decoding

https://github.com/lucidrains/speculative-decoding/blob/main/speculative_decoding/speculative_decoding_with_prophet.py

mha shape 변화
![image](https://github.com/jinuk0211/llm_project/assets/150532431/256147d7-f78f-41d9-a574-089a323b3b9a)

GQA, MHA 비교
![image](https://github.com/jinuk0211/llm_project/assets/150532431/1fb79469-dbc2-436c-87b2-64a61c8fa8f3)
