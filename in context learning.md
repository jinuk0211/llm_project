llm 만들기 가이드 - ICL

in-context learning

1. Why Can GPT Learn In-Context?
Language Models Implicitly Perform Gradient Descent as
Meta-Optimizers
https://arxiv.org/pdf/2212.10559

2. many-shot (2024년 4월 17일 구글 deepmind)
https://arxiv.org/pdf/2404.11018.pdf - context window 가 늘어나며 가능해짐
llama 3 - 8192, chatgpt4 - 128k
3. Chain of Thought without prompting
https://arxiv.org/abs/2402.10200

1.
![image](https://github.com/jinuk0211/llm_project/assets/150532431/3dc97499-bfce-41ea-a6fc-c706343efa64)


![image](https://github.com/jinuk0211/llm_project/assets/150532431/dd275103-00f3-4abe-8384-5772fede0744)
